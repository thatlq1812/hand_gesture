{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, color, feature, io\n",
    "from skimage.transform import resize \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hog_feature(image, pixel_per_cell=8):\n",
    "    \"\"\"\n",
    "    Compute HOG feature for a given image.\n",
    "\n",
    "    Args:\n",
    "        image: an image with object that we want to detect.\n",
    "        pixel_per_cell: number of pixels in each cell, an argument for hog descriptor.\n",
    "\n",
    "    Returns:\n",
    "        hogFeature: a vector of hog representation.\n",
    "        hogImage: an image representation of hog provided by skimage.\n",
    "    \"\"\"\n",
    "    hogFeature, hogImage = feature.hog(image,\n",
    "                                       pixels_per_cell=(pixel_per_cell, pixel_per_cell),\n",
    "                                       cells_per_block=(1, 1),\n",
    "                                       block_norm='L1',\n",
    "                                       visualize=True,\n",
    "                                       feature_vector=True)\n",
    "    return hogFeature, hogImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature, io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "def hog_feature(image, pixel_per_cell=8):\n",
    "    \"\"\"\n",
    "    Compute HOG feature for a given image.\n",
    "\n",
    "    Args:\n",
    "        image: an image with object that we want to detect.\n",
    "        pixel_per_cell: number of pixels in each cell, an argument for hog descriptor.\n",
    "\n",
    "    Returns:\n",
    "        hogFeature: a vector of hog representation.\n",
    "        hogImage: an image representation of hog provided by skimage.\n",
    "    \"\"\"\n",
    "    hogFeature, hogImage = feature.hog(image,\n",
    "                                       pixels_per_cell=(pixel_per_cell, pixel_per_cell),\n",
    "                                       cells_per_block=(3, 3),\n",
    "                                       block_norm='L1',\n",
    "                                       visualize=True,\n",
    "                                       feature_vector=True)\n",
    "    return hogFeature, hogImage\n",
    "\n",
    "def sliding_window(image, base_score, stepSize, windowSize, pixel_per_cell=8):\n",
    "    \"\"\" A sliding window that checks each different location in the image,\n",
    "        and finds which location has the highest hog score. The hog score is computed\n",
    "        as the dot product between the hog feature of the sliding window and the hog feature\n",
    "        of the template.\n",
    "\n",
    "    Args:\n",
    "        image: an np array of size (h,w).\n",
    "        base_score: hog representation of the object you want to find, an array of size (m,).\n",
    "        stepSize: an int of the step size to move the window.\n",
    "        windowSize: a pair of ints that is the height and width of the window.\n",
    "\n",
    "    Returns:\n",
    "        max_score: float of the highest hog score.\n",
    "        maxr: int of row where the max_score is found (top-left of window).\n",
    "        maxc: int of column where the max_score is found (top-left of window).\n",
    "        response_map: an np array of size (h,w).\n",
    "    \"\"\"\n",
    "    (max_score, maxr, maxc) = (0, 0, 0)\n",
    "    winH, winW = windowSize\n",
    "    H, W = image.shape\n",
    "    response_map = np.zeros((H // stepSize + 1, W // stepSize + 1))\n",
    "\n",
    "    for r in range(0, H - winH + 1, stepSize):\n",
    "        for c in range(0, W - winW + 1, stepSize):\n",
    "            window = image[r:r + winH, c:c + winW]\n",
    "            hogFeature, _ = hog_feature(window, pixel_per_cell)\n",
    "            score = np.dot(hogFeature, base_score)\n",
    "            response_map[r // stepSize, c // stepSize] = score\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                maxr, maxc = r, c\n",
    "\n",
    "    response_map = resize(response_map, (H, W))\n",
    "\n",
    "    return (max_score, maxr, maxc, response_map)\n",
    "\n",
    "# Load an example grayscale image\n",
    "image = io.imread('imagedata/00/01_palm/frame_00_01_0001.png', as_gray=True)\n",
    "\n",
    "# Define the template (for example, a hand region) - here we assume you have a predefined template\n",
    "# You need to load or define your own template for hand detection\n",
    "template = io.imread('path_to_template_image.png', as_gray=True)\n",
    "template_hog, _ = hog_feature(template)\n",
    "\n",
    "# Parameters\n",
    "stepSize = 20\n",
    "windowSize = template.shape\n",
    "\n",
    "# Perform sliding window to detect the region\n",
    "max_score, maxr, maxc, response_map = sliding_window(image, template_hog, stepSize, windowSize)\n",
    "\n",
    "# Draw a rectangle around the detected region\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "rect = plt.Rectangle((maxc, maxr), windowSize[1], windowSize[0], edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "# Display the response map\n",
    "plt.imshow(response_map, cmap='hot')\n",
    "plt.title('Response Map')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Đọc video từ camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Khởi tạo kích thước cửa sổ\n",
    "\n",
    "# Khởi tạo bộ phân đoạn nền\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    # Đọc từng khung hình\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Áp dụng bộ phân đoạn nền để phát hiện vật thể đang chuyển động\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    \n",
    "    # Xử lý để loại bỏ nhiễu và cải thiện kết quả\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Tìm các đường viền của các vật thể đã phát hiện\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Vẽ đường viền cho vật thể lớn nhất\n",
    "    if len(contours) > 0:\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(max_contour)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+w), (0, 255, 0), 2)\n",
    "    \n",
    "    # Hiển thị video kết quả\n",
    "    cv2.imshow('Motion Detection', frame)\n",
    "\n",
    "\n",
    "    \n",
    "    # Thoát nếu nhấn phím 'q'\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Giải phóng tài nguyên và đóng cửa sổ\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_hand_region(image):\n",
    "    # Chuyển đổi ảnh sang không gian màu HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \"\"\"\n",
    "    Trong không gian màu HSV, màu da thường có thể được xác định với Hue dao động từ khoảng 0 đến 20 độ (phụ thuộc vào nguồn tham khảo), \n",
    "    Saturation từ khoảng 30 đến 150, và Value từ khoảng 40 đến 200.\n",
    "    Tuy nhiên, các giá trị này có thể thay đổi tùy thuộc vào điều kiện ánh sáng và các yếu tố khác.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Xác định khoảng màu da trong không gian màu HSV\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "    \n",
    "    # Tạo mặt nạ màu da và áp dụng vào ảnh\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Loại bỏ nhiễu và tăng cường đường biên\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    edges = cv2.Canny(mask, 50, 100)\n",
    "    \n",
    "    # Tìm các vùng liên thông và vẽ bounding box\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # If contours square more than 5% and less than 20% of the image, draw the bounding box\n",
    "    image_w = image.shape[1]\n",
    "    image_h = image.shape[0]\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        if w < 0.1 * image_w or h < 0.1 * image_h:\n",
    "            continue\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc video từ camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Đọc từng khung hình\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Áp dụng hàm detect_hand_region để phát hiện vùng tay\n",
    "    output_frame = detect_hand_region(frame)\n",
    "    \n",
    "    # Hiển thị video kết quả\n",
    "    # output_frame = cv2.cvtColor(output_frame, cv2.COLOR_BGR2HSV)\n",
    "    cv2.imshow('Hand Detection', output_frame)\n",
    "    \n",
    "    # Thoát nếu nhấn phím 'q'\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Giải phóng tài nguyên và đóng cửa sổ\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load pre-trained Haar Cascade model for hand detection\n",
    "hand_cascade = cv2.CascadeClassifier('hand.xml')\n",
    "\n",
    "# Function to detect hands in an image\n",
    "def detect_hand(image):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    img_w = image.shape[1]\n",
    "    img_h = image.shape[0]\n",
    "    # Detect hands using Haar Cascade\n",
    "    hands = hand_cascade.detectMultiScale(gray, 1.05, 4, minSize=(int(img_w*0.05), int(img_h*0.05)), maxSize=(int(img_w*0.5), int(img_h*0.5)))\n",
    "    \n",
    "    # Draw rectangles around the hands\n",
    "    for (x, y, w, h) in hands:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example usage with camera\n",
    "cap = cv2.VideoCapture(0)  # Open the default camera (usually camera index 0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    output_frame = detect_hand(frame)\n",
    "    \n",
    "    cv2.imshow('Hand Detection', output_frame)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 58\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Resize frame for faster processing (optional)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Predict using HOG and SVM\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Assuming svm_model and data_scaler are already trained\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m prediction \u001b[38;5;241m=\u001b[39m predict_hog_svm(\u001b[43msvm_model\u001b[49m, data_scaler, frame)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Assuming 1 means hand is detected\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHand Detected\u001b[39m\u001b[38;5;124m'\u001b[39m, (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m50\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svm_model' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Load dataset (assuming you have a dataset of hand images and labels)\n",
    "# Replace with your actual dataset loading code\n",
    "# X_train, y_train = load_dataset()\n",
    "\n",
    "# Example of training using HOG features and SVM\n",
    "def train_hog_svm(X_train, y_train):\n",
    "    # Initialize HOG descriptor\n",
    "    hog_features = []\n",
    "    for img in X_train:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        hog_feature = hog(gray, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), transform_sqrt=True)\n",
    "        hog_features.append(hog_feature)\n",
    "    \n",
    "    hog_features = np.array(hog_features)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    hog_features = scaler.fit_transform(hog_features)\n",
    "    \n",
    "    # Train SVM classifier\n",
    "    svm = SVC(kernel='linear', probability=True)\n",
    "    svm.fit(hog_features, y_train)\n",
    "    \n",
    "    return svm, scaler\n",
    "\n",
    "# Example of predicting using trained SVM\n",
    "def predict_hog_svm(svm, scaler, image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hog_feature = hog(gray, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), transform_sqrt=True)\n",
    "    hog_feature = scaler.transform(hog_feature.reshape(1, -1))\n",
    "    \n",
    "    prediction = svm.predict(hog_feature)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example usage with camera\n",
    "cap = cv2.VideoCapture(0)  # Open the default camera (usually camera index 0)\n",
    "\n",
    "# Assuming you have trained the SVM beforehand and have the model and scaler ready\n",
    "# svm_model, data_scaler = train_hog_svm(X_train, y_train)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize frame for faster processing (optional)\n",
    "    # frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\n",
    "    \n",
    "    # Predict using HOG and SVM\n",
    "    # Assuming svm_model and data_scaler are already trained\n",
    "    prediction = predict_hog_svm(svm_model, data_scaler, frame)\n",
    "    \n",
    "    if prediction == 1:  # Assuming 1 means hand is detected\n",
    "        cv2.putText(frame, 'Hand Detected', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Hand Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Viola-Jones cascade for hand detection\n",
    "hand_cascade = cv2.CascadeClassifier('hand.xml')\n",
    "\n",
    "# Function to detect hands in an image using Viola-Jones\n",
    "def detect_hand_viola_jones(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect hands using Viola-Jones cascade\n",
    "    hands = hand_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Draw rectangles around the hands\n",
    "    for (x, y, w, h) in hands:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Example usage with camera\n",
    "cap = cv2.VideoCapture(0)  # Open the default camera (usually camera index 0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    output_frame = detect_hand_viola_jones(frame)\n",
    "    \n",
    "    cv2.imshow('Hand Detection (Viola-Jones)', output_frame)\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
