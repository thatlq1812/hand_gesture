{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import data, color, feature, io\n",
    "from skimage.transform import resize \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hog_feature(image, pixel_per_cell=8):\n",
    "    \"\"\"\n",
    "    Compute HOG feature for a given image.\n",
    "\n",
    "    Args:\n",
    "        image: an image with object that we want to detect.\n",
    "        pixel_per_cell: number of pixels in each cell, an argument for hog descriptor.\n",
    "\n",
    "    Returns:\n",
    "        hogFeature: a vector of hog representation.\n",
    "        hogImage: an image representation of hog provided by skimage.\n",
    "    \"\"\"\n",
    "    hogFeature, hogImage = feature.hog(image,\n",
    "                                       pixels_per_cell=(pixel_per_cell, pixel_per_cell),\n",
    "                                       cells_per_block=(1, 1),\n",
    "                                       block_norm='L1',\n",
    "                                       visualize=True,\n",
    "                                       feature_vector=True)\n",
    "    return hogFeature, hogImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import feature, io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "def hog_feature(image, pixel_per_cell=8):\n",
    "    \"\"\"\n",
    "    Compute HOG feature for a given image.\n",
    "\n",
    "    Args:\n",
    "        image: an image with object that we want to detect.\n",
    "        pixel_per_cell: number of pixels in each cell, an argument for hog descriptor.\n",
    "\n",
    "    Returns:\n",
    "        hogFeature: a vector of hog representation.\n",
    "        hogImage: an image representation of hog provided by skimage.\n",
    "    \"\"\"\n",
    "    hogFeature, hogImage = feature.hog(image,\n",
    "                                       pixels_per_cell=(pixel_per_cell, pixel_per_cell),\n",
    "                                       cells_per_block=(3, 3),\n",
    "                                       block_norm='L1',\n",
    "                                       visualize=True,\n",
    "                                       feature_vector=True)\n",
    "    return hogFeature, hogImage\n",
    "\n",
    "def sliding_window(image, base_score, stepSize, windowSize, pixel_per_cell=8):\n",
    "    \"\"\" A sliding window that checks each different location in the image,\n",
    "        and finds which location has the highest hog score. The hog score is computed\n",
    "        as the dot product between the hog feature of the sliding window and the hog feature\n",
    "        of the template.\n",
    "\n",
    "    Args:\n",
    "        image: an np array of size (h,w).\n",
    "        base_score: hog representation of the object you want to find, an array of size (m,).\n",
    "        stepSize: an int of the step size to move the window.\n",
    "        windowSize: a pair of ints that is the height and width of the window.\n",
    "\n",
    "    Returns:\n",
    "        max_score: float of the highest hog score.\n",
    "        maxr: int of row where the max_score is found (top-left of window).\n",
    "        maxc: int of column where the max_score is found (top-left of window).\n",
    "        response_map: an np array of size (h,w).\n",
    "    \"\"\"\n",
    "    (max_score, maxr, maxc) = (0, 0, 0)\n",
    "    winH, winW = windowSize\n",
    "    H, W = image.shape\n",
    "    response_map = np.zeros((H // stepSize + 1, W // stepSize + 1))\n",
    "\n",
    "    for r in range(0, H - winH + 1, stepSize):\n",
    "        for c in range(0, W - winW + 1, stepSize):\n",
    "            window = image[r:r + winH, c:c + winW]\n",
    "            hogFeature, _ = hog_feature(window, pixel_per_cell)\n",
    "            score = np.dot(hogFeature, base_score)\n",
    "            response_map[r // stepSize, c // stepSize] = score\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                maxr, maxc = r, c\n",
    "\n",
    "    response_map = resize(response_map, (H, W))\n",
    "\n",
    "    return (max_score, maxr, maxc, response_map)\n",
    "\n",
    "# Load an example grayscale image\n",
    "image = io.imread('imagedata/00/01_palm/frame_00_01_0001.png', as_gray=True)\n",
    "\n",
    "# Define the template (for example, a hand region) - here we assume you have a predefined template\n",
    "# You need to load or define your own template for hand detection\n",
    "template = io.imread('path_to_template_image.png', as_gray=True)\n",
    "template_hog, _ = hog_feature(template)\n",
    "\n",
    "# Parameters\n",
    "stepSize = 20\n",
    "windowSize = template.shape\n",
    "\n",
    "# Perform sliding window to detect the region\n",
    "max_score, maxr, maxc, response_map = sliding_window(image, template_hog, stepSize, windowSize)\n",
    "\n",
    "# Draw a rectangle around the detected region\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.imshow(image, cmap=plt.cm.gray)\n",
    "rect = plt.Rectangle((maxc, maxr), windowSize[1], windowSize[0], edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "# Display the response map\n",
    "plt.imshow(response_map, cmap='hot')\n",
    "plt.title('Response Map')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Đọc video từ camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Khởi tạo kích thước cửa sổ\n",
    "\n",
    "# Khởi tạo bộ phân đoạn nền\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    # Đọc từng khung hình\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Áp dụng bộ phân đoạn nền để phát hiện vật thể đang chuyển động\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    \n",
    "    # Xử lý để loại bỏ nhiễu và cải thiện kết quả\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Tìm các đường viền của các vật thể đã phát hiện\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Vẽ đường viền cho vật thể lớn nhất\n",
    "    if len(contours) > 0:\n",
    "        max_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(max_contour)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+w), (0, 255, 0), 2)\n",
    "    \n",
    "    # Hiển thị video kết quả\n",
    "    cv2.imshow('Motion Detection', frame)\n",
    "\n",
    "\n",
    "    \n",
    "    # Thoát nếu nhấn phím 'q'\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Giải phóng tài nguyên và đóng cửa sổ\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_hand_region(image):\n",
    "    # Chuyển đổi ảnh sang không gian màu HSV\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \"\"\"\n",
    "    Trong không gian màu HSV, màu da thường có thể được xác định với Hue dao động từ khoảng 0 đến 20 độ (phụ thuộc vào nguồn tham khảo), \n",
    "    Saturation từ khoảng 30 đến 150, và Value từ khoảng 40 đến 200.\n",
    "    Tuy nhiên, các giá trị này có thể thay đổi tùy thuộc vào điều kiện ánh sáng và các yếu tố khác.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Xác định khoảng màu da trong không gian màu HSV\n",
    "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "    \n",
    "    # Tạo mặt nạ màu da và áp dụng vào ảnh\n",
    "    mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "    \n",
    "    # Loại bỏ nhiễu và tăng cường đường biên\n",
    "    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n",
    "    edges = cv2.Canny(mask, 50, 100)\n",
    "    \n",
    "    # Tìm các vùng liên thông và vẽ bounding box\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # If contours square more than 5% and less than 20% of the image, draw the bounding box\n",
    "    image_w = image.shape[1]\n",
    "    image_h = image.shape[0]\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        if w < 0.1 * image_w or h < 0.1 * image_h:\n",
    "            continue\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        \n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc video từ camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Đọc từng khung hình\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Áp dụng hàm detect_hand_region để phát hiện vùng tay\n",
    "    output_frame = detect_hand_region(frame)\n",
    "    \n",
    "    # Hiển thị video kết quả\n",
    "    # output_frame = cv2.cvtColor(output_frame, cv2.COLOR_BGR2HSV)\n",
    "    cv2.imshow('Hand Detection', output_frame)\n",
    "    \n",
    "    # Thoát nếu nhấn phím 'q'\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Giải phóng tài nguyên và đóng cửa sổ\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Detect hands in the image\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m image_with_hands \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_hand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Display the image with hands detected\u001b[39;00m\n\u001b[0;32m     25\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHands Detected\u001b[39m\u001b[38;5;124m'\u001b[39m, image_with_hands)\n",
      "Cell \u001b[1;32mIn[48], line 8\u001b[0m, in \u001b[0;36mdetect_hand\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_hand\u001b[39m(image):\n\u001b[1;32m----> 8\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     hands \u001b[38;5;241m=\u001b[39m hand_cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray, scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m, minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, minSize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m30\u001b[39m))\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Draw rectangles around the hands\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load pre-trained Haar Cascade model for hand detection\n",
    "hand_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_hand.xml')\n",
    "\n",
    "# Function to detect hands in an image\n",
    "def detect_hand(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hands = hand_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Draw rectangles around the hands\n",
    "    for (x, y, w, h) in hands:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Load image\n",
    "image_path = 'D:\\S4_SU24\\CPV301\\hand_gesture\\test.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Detect hands in the image\n",
    "image_with_hands = detect_hand(image)\n",
    "\n",
    "# Display the image with hands detected\n",
    "cv2.imshow('Hands Detected', image_with_hands)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3915",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
